[["index.html", "Collostructional analysis: A short primer 1 Introduction 1.1 What you will learn in this tutorial 1.2 Prerequisites", " Collostructional analysis: A short primer Stefan Hartmann 2022-04-18 1 Introduction This tutorial provides a very short introduction to collostructional analysis, a family of methods that has been proposed by Stefanowitsch &amp; Gries (2003, 2005) and Gries &amp; Stefanowitsch (2004) and that has been widely adopted in corpus-based studies couched in a Construction Grammar framework. To illustrate how these methods can be used, I draw on the example of so-called “snowclones”, formulaic patterns with open slots that have attracted much attention in recent research on linguistic creativity (see this preprint for a discussion of the concept). I will first give a brief overview of the method and then discuss a few case studies. For the hands-on part of the tutorial, we will use R as well as Flach’s (2021) collostructions package. Note that the collostructions package is not (yet) available from CRAN. Please follow the installation instructions on the website to install it. 1.1 What you will learn in this tutorial In this tutorial, you will learn how to perform simple collostructional analyses using Flach’s (2021) collostructions package. I will also illustrate some basic data wrangling procedures (and some slightly more complex ones). 1.2 Prerequisites If you already know some R, it will be easier for you to follow this tutorial, but if not, don’t worry - you don’t have to understand everything immediately, the main goal of this tutorial is to give an impression of the steps you’ll have to take when doing collostructional analysis in R, without, however, discussing every single step in detail. In this tutorial, I will draw heavily on tidyverse syntax. The Tidyverse is a family of packages implementing a couple of functions that make common data science tasks a bit easier. In order to follow this tutorial, you should therefore have the tidyverse packages installed (you can install all of them via install.packages(\"tidyverse\")). "],["what-is-collostructional-analysis.html", "2 What is collostructional analysis?", " 2 What is collostructional analysis? Collostructional anlysis is a family of methods measuring associations and dissociations between (usually) words and (synactic) constructions. Its theoretical background is Construction Grammar, a theory of language that assumes that language can be exhaustively described as an inventory of form-meaning pairs. These form-meaning pairs are called constructions and exist at various levels of abstraction. For collostructional analysis, so-called partially filled constructions are particularly relevant, i.e. patterns with a lexically fixed part and a variable slot. Classic examples include idiomatic patterns like [X waiting to happen] or [The X-er the Y-er], but the method can be applied to much more abstract constructions as well (e.g. imperatives, discussed in Stefanowitsch &amp; Gries 2003). Collostructional analysis comes in three major variants: Simple collexeme analysis. This variant checks which slot fillers occur at above-chance level in one open slot of a construction, compared to the frequency of the same slot fillers in all other constructions. It is therefore particularly well-suited for patterns with one open slot, e.g. [X waiting to happen]. Distinctive collexeme analysis. This variant is used to compare two constructions by assessing which slot fillers occur with above-chance frequency in one construction compared to the other. Note that this variant only takes the frequencies of the slot fillers within the two constructions into account, i.e. the overall corpus frequencies are ignored. Covarying collexeme analysis. This variant assesses which slot fillers occur together with above-chance frequency in a construction with two open slots such as [The X-er the Y-er]. Again, only the frequency of the slot fillers within the construction is taken into account, i.e. the overall corpus frequencies are ignored. In Section @ref{cba}, I will give a brief conceptual overview of the ideas behind collostructional analysis, its potential, and its limitations. The subsequent sections contain hands-on tutorials for each of the three variants of collostructional analysis, including extensions of distinctive collexeme analysis that allow for taking more than two constructions into account. The subsequent sections discuss the three major variants of collostructional analysis using hands-on examples. The datasets used there can be downloaded from the “data” folder in this tutorial’s Github repository. "],["cba.html", "3 Conceptual background", " 3 Conceptual background As pointed out above, collostructional analysis is couched in (usage-based) Construction Grammar. This approach assumes that language users generalize over the linguistic units they encounter, thus gradually building up an inventory of form-meaning pairs (constructions). Constructions differ in schematicity, from fully lexically specific constructions (e.g. idioms like kick the bucket) to fully schematic ones (e.g. the so-called ditransitive construction, instantiated in She gave me a book or He baked me a cake). In-between these two poles, there are partially filled constructions like [the X-er the Y-er]. Collostructional analysis is particularly well-suited to investigate the open slots in the latter. Usage-based construction grammar assumes that our linguistic knowledge is based on generalizations over actual usage events. This means that frequency distributions can be highly informative about the semantics of constructions. For example, the ‘transfer’ semantics of the ditransitive construction is reflected (and potentially explained) by the fact that transfer verbs like give occur with above-chance frequency in this construction. That also explains the appeal of collostructional analysis - it provides a simple and intuitive way of assessing association patterns between words and constructions, thus providing clues to semantic aspects of a construction. However, there has also been criticism. I won’t go into detail here - instead, I’ll just mention some contentious aspects and point to some references for further reading. Bybee’s (2010) criticism is, broadly speaking, concerned with the cognitive plausibility of the measure. She criticizes that “Gries and colleagues argue for their statistical method but do not propose a cognitive mechanism that corresponds to their analysis” (Bybee 2010: 100) and argues that working with raw token frequencies might be a superior approach. See Gries (2014) for a reply. Schmid &amp; Küchenhoff’s (2013) criticism is more methodological in nature. Among other things, they take issue with the problem of “filling the fourth cell”, which we will encounter again in the hands-on tutorial. From a more theoretical perspective, they discuss a problem already hinted at by Bybee, namely that, als Kilgarriff (2007) famously put it, “language is never ever ever random”. "],["sca.html", "4 Simple collexeme analysis", " 4 Simple collexeme analysis Simple collexeme analysis investigates relationships between pairs of constructions, typically between a syntactic construction and a lexical item (Stefanowitsch 2013). It requires four values that are entered into a contingency table as shown in the table below: a) the frequency of a particular lexeme in a given construction; b) the frequency of all other lexemes in the same construction; c) the frequency of the lexeme in question outside of the given construction (i.e., in all other constructions); d) the frequency of all other lexemes outside of the construction. Table 4.1 summarizes the input that is needed for each of the four cells. Tab. 4.1: Frequency information needed for simple collexeme analysis Word lᵢ of Class L Other Words of Class L Total Construction c of Class C Freq. of L(lᵢ) in C(c) Frequency of L(¬lᵢ) in C(c) Total frequency of C(c) Other Constructions of class C Frequency of L(lᵢ) in C(cᵢ) Frequency of L(¬lᵢ) in C(¬cᵢ) Total frequency of C(¬c) Total Total Total frequency of L(lᵢ) Total frequency of L(¬lᵢ) Total frequency of C Admittedly, Table 4.1 is a bit hard to read, so it is probably helpful to work with examples. In the following, I will draw on so-called snowclones, i.e. extravagant formulaic patterns with an open slot and usually with a lexically fixed source phrase (see e.g. Bergs 2019, Ungerer &amp; Hartmann 2020), to illustrate collostructional analysis. Consider, for instance, [the mother of all X], which is often mentioned as a paradigm example of a snowclone, as there is a relatively clear source (Saddam Hussein’s “the mother of all bombs”), and it is used productively with a broad ramge of variation in the PP-internal noun slot. For getting hands-on experience, let us read in a dataset from the webcorpus ENCOW16A (Schäfer &amp; Bildhauer 2012). These data have been compiled and annotated by Tobias Ungerer and myself in the context of a joint project on snowclones. # read data moa &lt;- read_csv(&quot;data/mother_of_all_x/mother_of_all_ENCOW.csv&quot;) # exclude false hits moa &lt;- filter(moa, keep == &quot;y&quot;) # print summary table tibble( `number of tokens` = nrow(moa), `number of types` = length(unique(moa$lemma)), `Frequency of hangover` = length(which(moa$lemma==&quot;hangover&quot;)) ) %&gt;% kableExtra::kable() number of tokens number of types Frequency of hangover 4127 1669 62 (Note: If you see kableExtra::kable() or DT::datatable() anywhere in this tutorial, as in the function above, you can ignore it - those functions are just used to create the tables for the HTML file you’re currently reading.) A simple collexeme analysis can be used to check which lexemes occur with above-chance frequency in this slot, and which lexemes are rarer than we would expect, given a chance distribution. Take, for example, the lexical item hangover. To compute the collostruction strength of hangover, we have to fill the four cells as shown in Table 4.2. Tab. 4.2: Example contingency table for one slot filler Word lᵢ of Class L Other Words of Class L Total Construction c of Class C mother of all hangovers mother of all [¬hangover] Total frequency of C(c) Other Constructions of class C [¬mother of all] + hangover [¬mother of all] + [¬hangover] Total frequency of C(¬c) Total Total Total frequency of L(lᵢ) Total frequency of L(¬lᵢ) Total frequency of C The phrase the mother of all hangovers occurs 62 times in the corpus. Altogether, [mother of all X] is attested 4127 times in the corpus. The term hangover occurs a bit more than 18,000 times in the corpus, as a look at the ENCOW lemma list shows (available here.) This list can also be used to obtain the total frequency of all nouns in the corpus, which is a value that we will need later on. I’ve commented out the code, but you can uncomment it and try it out with the downloaded frequency list from webcorpora.org. # For this tutorial, I&#39;ve compiled a lemma list # containing only the lemmas attested in the &quot;mother of all&quot; # construction using this commented-out code: # library(vroom) # for fast processing of large files # encow &lt;- vroom(&quot;/Users/stefanhartmann/sciebo/Tutorials/collostructions_tutorial/data/encow16ax.lp.tsv&quot;, col_names = c(&quot;Lemma&quot;, &quot;POS&quot;, &quot;Freq&quot;)) ## only nouns: ## filter using the Penn POS tags used in ENCOW # encow[encow$POS %in% c(&quot;NN&quot;, # &quot;NNS&quot;, # &quot;NNP&quot;, # &quot;NNPS&quot;),]$Freq %&gt;% sum # 1805183579 # encow &lt;- filter(encow, Lemma %in% moa$lemma) # write_excel_csv(encow, &quot;data/moa_encow_freqs.csv&quot;) encow &lt;- read_csv(&quot;data/moa_encow_freqs.csv&quot;) # some lemmas occur twice with different pos tags, # so we first filter out nouns and then # summarize all that occur with different noun tags encow &lt;- encow[grep(&quot;N.*&quot;, encow$POS),] encow &lt;- encow %&gt;% group_by(Lemma) %&gt;% summarise( Lemma = Lemma, Freq = sum(Freq) ) %&gt;% unique() encow[which(encow$Lemma==&quot;hangover&quot;),] ## # A tibble: 1 × 2 ## # Groups: Lemma [1] ## Lemma Freq ## &lt;chr&gt; &lt;dbl&gt; ## 1 hangover 18482 Finally, we can compute the value for filling the fourth cell by obtaining the sum of all nouns in the corpus (from and then subtracting the values the fill the other cell of course). The total number of nouns in the corpus is 1805183579 (this is the number we get from summing up the frequencies of lemmas tagged as nouns in the entire lemma frequency list linked above). It is, however, not always easy to decide which value should enter the fourth cell (collostructional analysis has sometimes been criticized for this, see e.g. Schmid &amp; Küchenhoff 2013, Gries 2013). For example, regarding the [mother of all N] construction, one could discuss whether the value of the fourth cell should be the frequency of all other nouns in the corpus, including proper names, or whether proper names should be excluded (especially if they are not attested in the construction, which could be seen as evidence that the construction does not readily combine with proper names). Anyway, we will work with the full set of nouns here, without excluding proper names, as examples like the mother of all Karens seem perfectly possible. We can thus fill our cells as follows: # the mother of all hangovers: a &lt;- moa %&gt;% filter(lemma == &quot;hangover&quot;) %&gt;% nrow() # the mother of all ¬hangovers: b &lt;- moa %&gt;% filter(lemma != &quot;hangover&quot;) %&gt;% nrow() # ¬(the mother of all) hangovers: c &lt;- encow[which(encow$Lemma==&quot;hangover&quot;),]$Freq - a # ¬(the mother of all) ¬hangovers: # the sum of 1805183579 has been calculated from the full # ENCOW lemma frequency list d &lt;- 1805183579 - a - b - c # table: tibble( a = a, b = b, c = c, d = d ) %&gt;% kableExtra::kable() a b c d 62 4065 18420 1805161032 Using those values, we can now compute an association measure. For the sake of simplicity, we will use the chi-squared test here. Stefanowitsch &amp; Gries (2003) use the Fisher-Yates exact test. matrix(c(a,b,c,d), nrow = 2) %&gt;% chisq.test() ## Warning in chisq.test(.): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: . ## X-squared = 89392, df = 1, p-value &lt; 2.2e-16 Thus, according to the chi-squared test, the lexeme hangover occurs significantly more often in the [mother of all N] constructions than would be expected given a chance distribution. This also becomes clear if we compare the observed distribution to the expected one, which we can compute using R’s function for the chi-squared test, chisq.test(), which automatically computes expected frequencies. In the table below, the expected values are given in parantheses: # expected: exp &lt;- matrix(c(a,b,c,d), nrow = 2) %&gt;% chisq.test() ## Warning in chisq.test(.): Chi-squared approximation may be incorrect exp &lt;- exp$expected # observed: obs &lt;- matrix(c(a,b,c,d), nrow = 2) tibble( `x` = c(&quot;mother&quot;, &quot;not mother&quot;), hangover = c(paste0(obs[1], &quot; (&quot;, round(exp[1], digits = 3), &quot;)&quot;), paste0(obs[3], &quot; (&quot;, round(exp[3], digits = 3), &quot;)&quot;)), not_hangover = c(paste0(obs[2], &quot; (&quot;, round(exp[2], digits = 3), &quot;)&quot;), paste0(obs[4], &quot; (&quot;, round(exp[4], digits = 3), &quot;)&quot;)) ) %&gt;% kableExtra::kable() x hangover not_hangover mother 62 (0.042) 4065 (4126.958) not mother 18420 (18481.958) 1805161032 (1805160970.042) Of course we want to compute the collostruction strength not only for this one lexeme, but for all that occur in our dataset. We could write a function that automatizes what we’ve done so far; but luckily, Susanne Flach has already done that for us, and developed an entire R package dedicated to collostructional analysis. It is not (yet) available on CRAN but you can download it on her website. After installing it following the instructions given there, you should be able to load it: library(collostructions) The input that the package expects is simple - just check the documentation of the individual functions, for simple collexeme analysis ?collex, where it says that the first argument to be provided to the function should be “[a] data frame with types in a construction in column 1 (WORD), construction frequencies in column 2 (CXN.FREQ) and corpus frequencies in column 3 (CORP.FREQ).” Thus, we have to create a joint dataframe first - currently, the corpus frequencies for the first two cells are in the dataframe named moa, while the construction frequencies for the two remaining cells are in the dataframe encow. We first use the table function to get a dataframe containing the frequency of each lexeme in the [mother of all N] construction… # frequency within &quot;other of all&quot; tbl1 &lt;- moa %&gt;% select(lemma) %&gt;% # select the &quot;lemma&quot; column table %&gt;% # tabulate as_tibble() %&gt;% # convert to dataframe setNames(c(&quot;Lemma&quot;, &quot;Freq_mother&quot;)) # set the column names … and then join this table with the list of corpus frequencies, encow: tbl1 &lt;- left_join(tbl1, encow) ## Joining, by = &quot;Lemma&quot; Note that in this case, we don’t have to explicitly specify by which columns we want to join the data because there’s a column called “Lemma” in both dataframes, so the left_join function automatically uses the “Lemma” column as the variable to join by. If the columns had different names, say “Lemma” and “lemma”, we would have had to specify the columns explicitly, e.g. left_join(tbl1, encow, by = c(\"lemma\" = \"Lemma\")). Note that tbl1 conforms to the input required by the collex function, as cited above: The word in the first column, the construction frequency of each lexical item in the second, and its corpus frequency in the third. With the numbers in the second and third column of our new dataframe tbl1, we can fill the first two cells of our contingency table. Now you may wonder what happened to the third and fourth cells. Well, both can be easily computed as soon as we have the total number of words belonging to a certain category (in our case, nouns) in the corpus. We have seen above that ENCOW16A contains 1805183579 nouns. We can provide this value to collex as the corpsize argument. Thus, we have everything we need to do the simple collexeme analysis. However, the following line of code throws an error: collex(tbl1, corpsize = 1805183579) ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Lemma Freq_mother Freq ## 900 megamover 2 NA ## 96 beamprop 1 NA ## 164 branding 1 NA ## 228 categorical 1 NA ## 274 clear-cutter 1 NA ## 312 complementary 1 NA ## 386 cryptic 1 NA ## 502 dying 1 NA ## 504 earach 1 NA ## 646 gap-and-crap 1 NA ## 816 kicking 1 NA ## 873 mal 1 NA ## 994 nugshot 1 NA ## 1009 omnishamble 1 NA ## 1042 parliament-squarer 1 NA ## 1166 rapid 1 NA ## 1170 rattling 1 NA ## 1247 rollicking 1 NA ## 1310 shellacking 1 NA ## 1477 tamtram 1 NA ## 1506 thighburner 1 NA ## 1574 unclogging 1 NA ## 1575 uncontainable 1 NA ## 1578 unpleaseable 1 NA ## 1632 whooping 1 NA ## 1646 wobbly 1 NA ## Error in collex(tbl1, corpsize = 1805183579): ## Your input contains the above lines with incomplete cases. Why? The answer is simple, but maybe not obvious. In the moa dataframe, we are dealing with manually annotated data. Tobias and I have corrected the lemmatization of the individual lexemes because we wanted to have a reliable picture of their frequency in the construction. This is not possible for the entire corpus of course. This is why there are some lexemes that are attested in our [mother of all N] dataset, but not in the corpus (because there, they are lemmatized as “” or with a wrong lemma). As the error message above shows, they have a frequency value of NA. We can replace NA by 0 in such cases using the helpful replace_na function: tbl1 &lt;- tbl1 %&gt;% replace_na(list(Freq = 0)) Will this help? collex(tbl1, corpsize = 1805183579) ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Lemma Freq_mother Freq ## 900 megamover 2 0 ## 96 beamprop 1 0 ## 164 branding 1 0 ## 228 categorical 1 0 ## 274 clear-cutter 1 0 ## 312 complementary 1 0 ## 386 cryptic 1 0 ## 502 dying 1 0 ## 504 earach 1 0 ## 646 gap-and-crap 1 0 ## 816 kicking 1 0 ## 873 mal 1 0 ## 994 nugshot 1 0 ## 1009 omnishamble 1 0 ## 1042 parliament-squarer 1 0 ## 1166 rapid 1 0 ## 1170 rattling 1 0 ## 1247 rollicking 1 0 ## 1310 shellacking 1 0 ## 1477 tamtram 1 0 ## 1506 thighburner 1 0 ## 1574 unclogging 1 0 ## 1575 uncontainable 1 0 ## 1578 unpleaseable 1 0 ## 1632 whooping 1 0 ## 1646 wobbly 1 0 ## Error in collex(tbl1, corpsize = 1805183579): ## For the above item(s), the frequency in the construction is larger than the frequency in the corpus. Not really, as the function complains about the same rows again - rightly so, because it doesn’t make much sense to look at items that are, at least according to the lemmatization of the corpus, not attested at all in our database. It would be like counting the number of apples drawn from a bag of pears. There are several ways to solve this problem. If we wanted to have very precise results, we could manually query the corpus for the tokens that are only attested in the [mother of all N] database, ideally taking spelling variants into account, thus obtaining the corpus frequency for each of the affected lexical items individually. Given the low frequency of the affected lemmas in the construction, however, this seems rather unnecessary. Personally, I tend to exclude those items (and make this transparent when I’m reporting the results in a paper). tbl1 &lt;- tbl1[which(tbl1$Freq_mother &lt;= tbl1$Freq),] Now we have a table that should work as input for collex: tbl1 %&gt;% collex() %&gt;% DT::datatable() ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used Et voilà, we have a collexeme table we can work with - for example, by interpreting the results: Among other things, it is interesting to see that the items most strongly attracted to the construction tend to have a negative semantic prosody, with many of them being rather colloquiual, which might contribute to the “extravagant” nature of the construction. "],["distinctive-collexeme-analysis.html", "5 Distinctive collexeme analysis", " 5 Distinctive collexeme analysis Construction Grammar has always been quite interested in so-called alternations, e.g. the so-called dative alternation between the ditransitive (or double-object) construction (I gave her the book) and the to-dative construction (I gave the book to her). Such alternations are interesting for a variety of reasons, one of them being the fact that Construction Grammarians tend to assume a “principle of non synonymy” (see Goldberg 1995, but see Uhrig 2015 for critical remarks). Comparing the slot fillers across constructions can help to get an impression of potentially quite subtle meaning differences between such near-synonymous constructions. To stick with the example of “snowclones”, we could say that [the mother of all N] competes with the less “snowclony” [the ADJ-est N ever/of all times], even though an obvious differences between both constructions is of course that the [mother of all N] construction, while evoking an implicit superlative, leaves the way in which something is superlative underspecified. Despite those differences, let us compare the N slots in the [mother of all N] construction and the [ADJ-est N ever] construction using distinctive collexeme analysis. The basic logic of distinctive collexeme analysis is very similar to that of simple collexeme analysis, except that we don’t compare construction-internal and construction-external frequencies, but instead frequencies within two constructions. While this entails the obvious shortcoming that corpus frequencies are not taken into account, it can help get a clearer picture of the commonalities and differences of the two constructions. The basic contingency table for distinctive collemexe analysis is shown in Table @ref(tab:dca_tbl). ## New names: ## * `` -&gt; ...1 (#tab:dca_tbl)Contingency table for distinctive collexeme analysis Word lᵢ of Class L Other Words of Class L Total Construction c₁ of Class C Freq. of L(lᵢ) in C(c₁) Frequency of L(¬lᵢ) in C(c₁) Total frequency of C(c₁) Construction c₂ of Class C Frequency of L(lᵢ) in C(c₂) Frequency of L(¬lᵢ) in C(¬c₂) Total frequency of C(¬c₂) Total Total Total frequency of L(lᵢ) in C(c₁, c₂) Total frequency of L(¬lᵢ) in C(c₁, c₂) Total frequency of C(c₁, c₂) Again, this table is easier to understand if we translate it to our example, i.e. the comparison between [mother of all N] and [ADJ-est N ever] (where we ignore the ADJ slot and only focus on the N slot): ## New names: ## * `` -&gt; ...1 (#tab:dca_tbl2)Example contingency table for distinctive collexeme analysis Word lᵢ of Class L Other Words of Class L Total Construction c₁ of Class C Frequency of \"hangover\" in the \"mother of all\" cxn Frequency of all other nouns in the \"mother of all\" cxn Total frequency of \"mother of all\" Construction c₂ of Class C Frequency of \"hangover\" in the \"ADJ-est N ever\" construction Frequency of all other nouns in the \"ADJ-est N ever\" construction Total frequency of \"ADJ-est N ever\" Total Total frequency of \"hangover\" in both constructions Total frequency of all other nouns in the two constructions Total frequency of both cxns Let us briefly work through this example. We already have the data for [mother of all N], but we still need the data for [ADJ-est N ever]. To obtain them, I have queried ENCOW16A via NoSketchEngine using the search query \"[tT]he\" \".*est\" [tag=\"N.*\"] \"ever\". We import the results using the getNSE function from the package concordances, which is available via Github and which you can install using the following command: if(!is.element(&quot;devtools&quot;, installed.packages())) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;hartmast/concordances&quot;) Now we load the package using library(concordances) And we read in the data: ever &lt;- getNSE(&quot;data/adj_est_n_ever/the_ADJest_N_ever.xml&quot;, xml = TRUE, tags = TRUE, context_tags = FALSE) ## Processing tags in the keyword column ... Now we have to extract the lemmas in the noun slot. The original data have lemma tags, and getNSE has extracted them for us to a separate column (named “Tag2_Key”): head(ever) ## Left ## 1 . ... to Failed States Widely acclaimed as ## 2 to be found in Destiny of the Daleks as part of ## 3 into the world . The 175 years of his life would be ## 4 a huge favorite ? All he did this season was have ## 5 Hitchcock and Bloch . The first Psycho is one of ## 6 , you have n&#39;t seen anything yet ! Quite possibly ## Key ## 1 the best television ever ## 2 the biggest rouse ever ## 3 the greatest blessing ever ## 4 the best year ever ## 5 the darkest works ever ## 6 the best sequel ever ## Right ## 1 , US crime saga The Wire finally arrives on ## 2 . Davros , and the Doctor for that matter , ## 3 bestowed upon &quot; the families of the earth &quot; . The ## 4 for a quarterback in breaking NFL records for ## 5 committed to celluloid , and much of its ## 6 written , &quot; Sony Talks About PSP &quot; takes ## Key_with_anno ## 1 the /the best /good television /television ever /ever ## 2 the /the biggest /big rouse /rouse ever /ever ## 3 the /the greatest /great blessing /blessing ever /ever ## 4 the /the best /good year /year ever /ever ## 5 the /the darkest /dark works /work ever /ever ## 6 the /the best /good sequel /sequel ever /ever ## Tag1_Key Tag2_Key ## 1 the best television ever the good television ever ## 2 the biggest rouse ever the big rouse ever ## 3 the greatest blessing ever the great blessing ever ## 4 the best year ever the good year ever ## 5 the darkest works ever the dark work ever ## 6 the best sequel ever the good sequel ever Thus, we just have to extract the third word in each row of the Tag2_Key column. Extracting the third word from a single character string is simple using the strsplit command: unlist(strsplit(&quot;The best function ever&quot;, split = &quot; &quot;))[3] # whitespace as separator ## [1] &quot;function&quot; We can apply this function over an entire vector, or in our case: a column of a dataframe, using sapply: ever_n &lt;- sapply(1:nrow(ever), function(i) unlist(strsplit(ever$Tag2_Key[i], &quot; &quot;))[3]) From this list of nouns, we can create a frequency table: ever_n_tbl &lt;- table(ever_n) %&gt;% as_tibble() %&gt;% setNames(c(&quot;Lemma&quot;, &quot;Freq_ever&quot;)) %&gt;% arrange(desc(Freq_ever)) # arrange in descending order head(ever_n_tbl) ## # A tibble: 6 × 2 ## Lemma Freq_ever ## &lt;chr&gt; &lt;int&gt; ## 1 thing 3724 ## 2 (unknown) 2362 ## 3 hero 1719 ## 4 holiday 1673 ## 5 attendance 1466 ## 6 game 1250 We can now merge this with our table tbl1 compiled in Section 4. tbl1 &lt;- left_join(tbl1, ever_n_tbl) ## Joining, by = &quot;Lemma&quot; head(tbl1) ## # A tibble: 6 × 4 ## Lemma Freq_mother Freq Freq_ever ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ab 1 11706 NA ## 2 abomination 4 16830 2 ## 3 accelerator 1 25388 1 ## 4 accent 1 101851 10 ## 5 accident 2 451655 8 ## 6 achievement 1 355966 14 There are of course some lexemes that occur only in one construction and not in the other, hence the NA’s in the output. We can remove them using the replace_na function, and as this function takes a list as its argument, we can so so simultaneously for multiple columns: tbl1 &lt;- tbl1 %&gt;% replace_na(list(Freq_mother = 0, Freq_ever = 0)) Now we have all we need for a distinctive collexeme analysis. In the collostructions package, we can use collex.dist to perform a distinctive collexeme analysis. According to the package vignette (see ?collex.dist), we have two options to pass our data to the function: “EITHER as aggregated frequencies with the types in column A (WORD), and the frequencies of WORD in the first construction in column 2 and in the frequencies of WORD in the second construction in column 3, OR as raw data, i.e., one observation per line, where column 1 must contain the construction types and column 2 must contain the collexeme.” As we already have the frequency list, we go for the first option. In fact, we simply have to select the relevant columns from the tbl1 dataframe and can pass them to collex.dist. tbl1 %&gt;% select(Lemma, Freq_mother, Freq_ever) %&gt;% collex.dist() %&gt;% DT::datatable() ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used Again, we see that certain words like battle, hangover, crisis occur much more often in the [mother of all N] construction than in the [ADJ-est N ever] construction. We can reverse the list to take a look at the collexemes that rather tend to occur in the [ADJ-est N ever] construction: tbl1 %&gt;% select(Lemma, Freq_mother, Freq_ever) %&gt;% collex.dist(reverse = TRUE) %&gt;% DT::datatable() ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used Comparing the distinctive collexemes of both constructions is quite instructive - overall, it seems as if [mother of all N] tends to combine more with abstract concepts and nouns denoting events, while [ADJ-est N ever] combines with terms that denote more concrete/individuated concepts like persons, objects, cultural products etc. "],["covarying-collexeme-analysis.html", "6 Covarying-collexeme analysis", " 6 Covarying-collexeme analysis Last but not least, let’s take a look at covarying-collexeme analysis. As the name already reveals, this method is about “covarying collexemes”. Many constructions have more than one open slot - think of the [the ADJ-est N ever] construction that we’ve just encountered, or the “snowclone” [X BE the new Y]. In such cases, it can be interesting to take a look at the constructions that occur together in the same construction. Tab. 6.1: Frequency information needed for distinctive collexeme analysis Word l₁ in slot s₁ of construction C Other words in slot s₁ of construction C Total Construction c of Class C Freq. of s₁(l₁) and s₂(l₂) in C Freq. of s₁(¬l₁) and s₂(l₂) in C Total frequency of s₂(l₂) in C Other Constructions of class C Freq. of s₁(l₁) and s₂(¬l₂) in C Freq. of s₁(¬l₁) and s₂(¬l₂) in C Total frequency of s₁(l₁) in C Total Total frequency of s₁(l₁) in C Total frequency of s₁(¬l₁) in C Total frequency of C Again, it helps to illustrate this with a concrete example, in this case the [X BE the new Y] construction with instances like Scientists are the new rock stars: Tab. 6.2: Example contingency table for distinctive collexeme analysis Word l₁ in slot s₁ of construction C Other words in slot s₁ of construction C Total Construction c of Class C Frequency of \"scientists are the new rock stars\" Frequency of \"X are the new rock stars\", with X being any other lexeme than \"scientist\" Total frequency of \"X are the new rock stars\" Other Constructions of class C Frequency of \"scientists are the new Y\", with Y being any other lexeme than \"rock stars\" Frequency of \"X BE the new Y\", with X and Y being any other lexeme than \"scientist\" and \"rock star\", respectively Total frequency of \"X BE the new Y\", with Y being any other lexeme than \"rock stars\" Total Total frequency of \"Scientists are the new Y\" Total frequency of \"X BE the new Y\", with X being any other lexeme than \"scientist\" Total frequency of \"X BE the new Y\" As it happens, Tobias Ungerer and I have investigated this construction as well, so we can use our dataset, which is again drawn from ENCOW16A: xnewy &lt;- read_csv(&quot;data/x_is_the_new_y/ENCOW_x_is_the_new_y_without_false_hits.csv&quot;) ## Rows: 5082 Columns: 47── Column specification ────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (45): Metatag1, Metatag2, Metatag3, Left, Key, Right, Key_with_ann... ## dbl (2): No, Metatag4 ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. All we need in this case are the Lemma_x and Lemma_y columns in the table, which we can easily pass to collex.covar from the collostructions package - like collex.dist, it accepts both frequency tables and word lists with one observation per line as input. So we just have to select the relevant columns and pass the resulting dataframe to the collex.covar function. First, however, we convert the columns Lemma_x and Lemma_y to lowercase, because the use of capitalization is a bit inconsistent in those columns.1 # convert to lowercase xnewy$Lemma_x &lt;- tolower(xnewy$Lemma_x) xnewy$Lemma_y &lt;- tolower(xnewy$Lemma_y) # collexeme analysis xnewy %&gt;% select(Lemma_x, Lemma_y) %&gt;% collex.covar(raw = T) %&gt;% DT::datatable() ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used Unsurprisingly, the terms that tend to occur together come from similar domains, as in small is the new big, x is the new y, transparency is the new objectivity. Doll-art is the new anarchy is a bit surprising, but that might be an artifact of the corpus, which sometimes contains the same text(s) multiple times, which can distort the results. A quick look at the corpus data confirms this assumption: Fig. 6.1: Concordance for “doll art” in ENCOW This is a reminder that the results of collostructional analysis are of course just as good as the database that you are using, and that aspects like corpus composition and potential biases in the data have to be taken into account. Apart from that, however, the results are again quite instructive, as they show which terms tend to go together (e.g. color terms with other color terms; abstract concepts with abstract concepts, etc.) and which don’t. Summing up, then, collostructional analysis can yield very revealing results that help us understand the semantics of linguistic constructions, as well as the restrictions they may be subject to. In fact, it is a bit more complex because we actually used capitalization for marking heads of phrases, and to get more meaningful results, we would have to extract those heads first. We won’t do this here to keep things simple, but you can read more here if you’re interested.↩︎ "],["references.html", "7 References", " 7 References Bergs, Alexander. 2019. What, if anything, is linguistic creativity? Gestalt Theory 41(2). 173–183. Flach, Susanne. 2021. collostructions: An R Implementation for the Family of Collostructional Methods. www.sfla.ch. Goldberg, Adele E. 1995. Constructions: A Construction Grammar Approach to Argument Structure. Chicago, London: The University of Chicago Press. Gries, Stefan Th. 2015. More (old and new) misunderstandings of collostructional analysis: On Schmid and Küchenhoff (2013). Cognitive Linguistics 26(3). 505–536. Gries, Stefan Th. 2019. 15 years of collostructions: Some long overdue additions/corrections (to/of actually all sorts of corpus-linguistics measures). International Journal of Corpus Linguistics 24(3). 385–412. Gries, Stefan Th. &amp; Anatol Stefanowitsch. 2004. Extending Collostructional Analysis: A Corpus-Based Perspective on “Alternations.” International Journal of Corpus Linguistics 9(1). 97–129. Hilpert, Martin. 2010. The force dynamics of English complement clauses: A collostructional analysis. In Dylan Glynn &amp; Kerstin Fischer (eds.), Quantitative Methods in Cognitive Semantics: Corpus-Driven Approaches, 155–178. Berlin, New York: De Gruyter. Hilpert, Martin. 2012. Diachronic Collostructional Analysis Meets the Noun Phrase: Studying MANY A NOUN in COHA. In Terttu Nevalainen &amp; Elizabeth Closs Traugott (eds.), The Oxford Handbook of the History of English, 233–244. Oxford: Oxford University Press. Küchenhoff, Helmut &amp; Hans-Jörg Schmid. 2015. Reply to “More (old and new) misunderstandings of collostructional analysis: On Schmid &amp; Küchenhoff” by Stefan Th. Gries. Cognitive Linguistics 26(3). 537–547. Schäfer, Roland &amp; Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool Chain. In Nicoletta Calzolari, Khalid Choukri, Terry Declerck, Mehmet Uğur Doğan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk &amp; Stelios Piperidis (eds.), Proceedings of LREC 2012, 486–493. Schmid, Hans-Jörg &amp; Helmut Küchenhoff. 2013. Collostructional Analysis and other Ways of Measuring Lexicogrammatical Attraction: Theoretical Premises, Practical Problems and Cognitive Underpinnings. Cognitive Linguistics 24(3). 531–577. Stefanowitsch, Anatol. 2013. Collostructional Analysis. In Thomas Hoffmann &amp; Graeme Trousdale (eds.), The Oxford Handbook of Construction Grammar, 290–306. Oxford: Oxford University Press. Stefanowitsch, Anatol &amp; Stefan Th. Gries. 2003. Collostructions: Investigating the Interaction of Words and Constructions. International Journal of Corpus Linguistics 8(2). 209–243. Uhrig, Peter. 2015. Why the Principle of No Synonymy is Overrated. Zeitschrift für Anglistik und Amerikanistik 63(3). 323–337. Ungerer, Tobias &amp; Stefan Hartmann. 2020. Delineating extravagance: Assessing speakers’ perceptions of imaginative constructional patterns. Belgian Journal of Linguistics 34. 345–356. Wiechmann, Daniel. 2008. On the computation of collostruction strength: Testing measures of association as expressions of lexical bias. Corpus Linguistics and Linguistic Theory 4(2). 253–290. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

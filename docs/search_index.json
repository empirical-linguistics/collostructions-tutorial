[["index.html", "Collostructional analysis: A short primer 1 Introduction", " Collostructional analysis: A short primer Stefan Hartmann 2022-04-15 1 Introduction This tutorial provides a very short introduction to collostructional analysis, a family of methods that has been proposed by Stefanowitsch &amp; Gries (2003, 2005) and Gries &amp; Stefanowitsch (2004) and that has been widely adopted in corpus-based studies couched in a Construction Grammar framework. I will first give a brief overview of the method and then discuss a few case studies. For the hands-on part of the tutorial, we will use R as well as Flach’s (2017) collostructions package. Note that the collostructions package is not (yet) available from CRAN. Please follow the installation instructions on the website to install it. "],["what-is-collostructional-analysis.html", "2 What is collostructional analysis?", " 2 What is collostructional analysis? Collostructional anlysis is a family of methods measuring associations and dissociations between (usually) words and (synactic) constructions. Its theoretical background is Construction Grammar, a theory of language that assumes that language can be exhaustively described as an inventory of form-meaning pairs. These form-meaning pairs are called constructions and exist at various levels of abstraction. For collostructional analysis, so-called partially filled constructions are particularly relevant, i.e. patterns with a lexically fixed part and a variable slot. Classic examples include idiomatic patterns like [X waiting to happen] or [The X-er the Y-er], but the method can be applied to much more abstract constructions as well (e.g. imperatives, discussed in Stefanowitsch &amp; Gries 2003). Collostructional analysis comes in three major variants: Simple collexeme analysis. This variant checks which slot fillers occur at above-chance level in one open slot of a construction, compared to the frequency of the same slot fillers in all other constructions. It is therefore particularly well-suited for patterns with one open slot, e.g. [X waiting to happen]. Distinctive collexeme analysis. This variant is used to compare two constructions by assessing which slot fillers occur with above-chance frequency in one construction compared to the other. Note that this variant only takes the frequencies of the slot fillers within the two constructions into account, i.e. the overall corpus frequencies are ignored. Covarying collexeme analysis. This variant assesses which slot fillers occur together with above-chance frequency in a construction with two open slots such as [The X-er the Y-er]. Again, only the frequency of the slot fillers within the construction is taken into account, i.e. the overall corpus frequencies are ignored. In Section 2, I will give a brief conceptual overview of the ideas behind collostructional analysis, its potential, and its limitations. The subsequent sections contain hands-on tutorials for each of the three variants of collostructional analysis, including extensions of distinctive collexeme analysis that allow for taking more than two constructions into account. "],["conceptual-background.html", "3 Conceptual background", " 3 Conceptual background As pointed out above, collostructional analysis is couched in (usage-based) Construction Grammar. This approach assumes that language users generalize over the linguistic units they encounter, thus gradually building up an inventory of form-meaning pairs (constructions). Constructions differ in schematicity, from fully lexically specific constructions (e.g. idioms like kick the bucket) to fully schematic ones (e.g. the so-called ditransitive construction, instantiated in She gave me a book or He baked me a cake). In-between these two poles, there are partially filled constructions like [the X-er the Y-er]. Collostructional analysis is particularly well-suited to investigate the open slots in the latter. Usage-based construction grammar assumes that our linguistic knowledge is based on generalizations over actual usage events. This means that frequency distributions can be highly informative about the semantics of constructions. For example, the ‘transfer’ semantics of the ditransitive construction is reflected (and potentially explained) by the fact that transfer verbs like give occur with above-chance frequency in this construction. That also explains the appeal of collostructional analysis - it provides a simple and intuitive way of assessing association patterns between words and constructions, thus providing clues to semantic aspects of a construction. However, there has also been criticism. I won’t go into detail here - instead, I’ll just mention some contentious aspects and point to some references for further reading. Bybee’s (2010) criticism is, broadly speaking, concerned with the cognitive plausibility of the measure. She criticizes that “Gries and colleagues argue for their statistical method but do not propose a cognitive mechanism that corresponds to their analysis” (Bybee 2010: 100) and argues that working with raw token frequencies might be a superior approach. See Gries (2014) for a reply. Schmid &amp; Küchenhoff’s (2013) criticism is more methodological in nature. Among other things, they take issue with the problem of “filling the fourth cell”, which we will encounter again in the hands-on tutorial. From a more theoretical perspective, they discuss a problem already hinted at by Bybee, namely that, als Kilgarriff (2007) famously put it, “language is never ever ever random”. "],["simple-collexeme-analysis.html", "4 Simple collexeme analysis", " 4 Simple collexeme analysis Simple collexeme analysis investigates relationships between pairs of constructions, typically between a syntactic construction and a lexical item (Stefanowitsch 2013). It requires four values that are entered into a contingency table as shown in the table below: a) the frequency of a particular lexeme in a given construction; b) the frequency of all other lexemes in the same construction; c) the frequency of the lexeme in question outside of the given construction (i.e., in all other constructions); d) the frequency of all other lexemes outside of the construction. Table 4.1 summarizes the input that is needed for each of the four cells. Tab. 4.1: Frequency information needed for simple collexeme analysis Word lᵢ of Class L Other Words of Class L Total Construction c of Class C Freq. of L(lᵢ) in C(c) Frequency of L(¬lᵢ) in C(c) Total frequency of C(c) Other Constructions of class C Frequency of L(lᵢ) in C(cᵢ) Frequency of L(¬lᵢ) in C(¬cᵢ) Total frequency of C(¬c) Total Total Total frequency of L(lᵢ) Total frequency of L(¬lᵢ) Total frequency of C Admittedly, Table 4.1 is a bit hard to read, so it is probably helpful to work with examples. In the following, I will draw on so-called snowclones, i.e. extravagant formulaic patterns with an open slot and usually with a lexically fixed source phrase (see e.g. Bergs 2019, Ungerer &amp; Hartmann 2020), to illustrate collostructional analysis. Consider, for instance, [the mother of all X], which is often mentioned as a paradigm example of a snowclone, as there is a relatively clear source (Saddam Hussein’s “the mother of all bombs”), and it is used productively with a broad ramge of variation in the PP-internal noun slot. For getting hands-on experience, let us read in a dataset from the webcorpus ENCOW16A (Schäfer &amp; Bildhauer 2012). These data have been compiled and annotated by Tobias Ungerer and myself in the context of a joint project on snowclones. # read data moa &lt;- read_csv(&quot;data/mother_of_all_x/mother_of_all_ENCOW.csv&quot;) # exclude false hits moa &lt;- filter(moa, keep == &quot;y&quot;) # print summary table tibble( `number of tokens` = nrow(moa), `number of types` = length(unique(moa$lemma)), `Frequency of hangover` = length(which(moa$lemma==&quot;hangover&quot;)) ) %&gt;% kableExtra::kable() number of tokens number of types Frequency of hangover 4127 1669 62 A simple collexeme analysis can be used to check which lexemes occur with above-chance frequency in this slot, and which lexemes are rarer than we would expect, given a chance distribution. Take, for example, the lexical item hangover. To compute the collostruction strength of hangover, we have to fill the four cells as shown in Table 4.2. Tab. 4.2: Example contingency table for one slot filler Word lᵢ of Class L Other Words of Class L Total Construction c of Class C mother of all hangovers mother of all [¬hangover] Total frequency of C(c) Other Constructions of class C [¬mother of all] + hangover [¬mother of all] + [¬hangover] Total frequency of C(¬c) Total Total Total frequency of L(lᵢ) Total frequency of L(¬lᵢ) Total frequency of C The phrase the mother of all hangovers occurs 62 times in the corpus. Altogether, [mother of all X] is attested 4127 times in the corpus. The term hangover occurs a bit more than 18,000 times in the corpus, as a look at the ENCOW lemma list shows (available here.) # For this tutorial, I&#39;ve compiled a lemma list # containing only the lemmas attested in the &quot;mother of all&quot; # construction using this commented-out code: library(vroom) # encow &lt;- vroom(&quot;/Users/stefanhartmann/sciebo/Tutorials/collostructions_tutorial/data/encow16ax.lp.tsv&quot;, col_names = c(&quot;Lemma&quot;, &quot;POS&quot;, &quot;Freq&quot;)) # encow[grep(&quot;N.*&quot;, encow$POS),]$Freq %&gt;% sum # 4091470964 # encow &lt;- filter(encow, Lemma %in% moa$lemma) # write_excel_csv(encow, &quot;data/moa_encow_freqs.csv&quot;) encow &lt;- read_csv(&quot;data/moa_encow_freqs.csv&quot;) # some lemmas occur twice with different pos tags, # so we first filter out nouns and then # summarize all that occur with different noun tags encow &lt;- encow[grep(&quot;N.*&quot;, encow$POS),] encow &lt;- encow %&gt;% group_by(Lemma) %&gt;% summarise( Lemma = Lemma, Freq = sum(Freq) ) %&gt;% unique() encow[which(encow$Lemma==&quot;hangover&quot;),] ## # A tibble: 1 × 2 ## # Groups: Lemma [1] ## Lemma Freq ## &lt;chr&gt; &lt;dbl&gt; ## 1 hangover 18482 Finally, we can compute the value for filling the fourth cell by obtaining the sum of all nouns in the corpus (from and then subtracting the values the fill the other cell of course). The total number of nouns in the corpus is 6.992691^{8}: sum(encow$Freq) ## [1] 699269104 It is, however, not always easy to decide which value should enter the fourth cell (collostructional analysis has sometimes been criticized for this, see e.g. Schmid &amp; Küchenhoff 2013, Gries 2013). For example, regarding the [mother of all N] construction, one could discuss whether the value of the fourth cell should be the frequency of all other nouns in the corpus, including proper names, or whether proper names should be excluded (especially if they are not attested in the construction, which could be seen as evidence that the construction does not readily combine with proper names). Anyway, we will work with the full set of nouns here, without excluding proper names, as examples like the mother of all Karens seem perfectly possible. We can thus fill our cells as follows: # the mother of all hangovers: a &lt;- moa %&gt;% filter(lemma == &quot;hangover&quot;) %&gt;% nrow() # the mother of all ¬hangovers: b &lt;- moa %&gt;% filter(lemma != &quot;hangover&quot;) %&gt;% nrow() # ¬(the mother of all) hangovers: c &lt;- encow[which(encow$Lemma==&quot;hangover&quot;),]$Freq - a # ¬(the mother of all) ¬hangovers: # the sum of 4091470964 has been calculated from the full # ENCOW lemma frequency list d &lt;- 4091470964 - a - b - c # table: tibble( a = a, b = b, c = c, d = d ) %&gt;% kableExtra::kable() a b c d 62 4065 18420 4091448417 Using those values, we can now compute an association measure. For the sake of simplicity, we will use the chi-squared test here. Stefanowitsch &amp; Gries (2003) use the Fisher-Yates exact test. matrix(c(a,b,c,d), nrow = 2) %&gt;% chisq.test() ## Warning in chisq.test(.): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: . ## X-squared = 202761, df = 1, p-value &lt; 2.2e-16 Thus, according to the chi-squared test, the lexeme hangover occurs significantly more often in the [mother of all N] constructions than would be expected given a chance distribution. This also becomes clear if we compare the observed distribution to the expected one, which we can compute using R’s function for the chi-squared test, chisq.test(), which automatically computes expected frequencies. In the table below, the expected values are given in parantheses: # expected: exp &lt;- matrix(c(a,b,c,d), nrow = 2) %&gt;% chisq.test() ## Warning in chisq.test(.): Chi-squared approximation may be incorrect exp &lt;- exp$expected # observed: obs &lt;- matrix(c(a,b,c,d), nrow = 2) tibble( `x` = c(&quot;mother&quot;, &quot;not mother&quot;), hangover = c(paste0(obs[1], &quot; (&quot;, round(exp[1], digits = 3), &quot;)&quot;), paste0(obs[3], &quot; (&quot;, round(exp[3], digits = 3), &quot;)&quot;)), not_hangover = c(paste0(obs[2], &quot; (&quot;, round(exp[2], digits = 3), &quot;)&quot;), paste0(obs[4], &quot; (&quot;, round(exp[4], digits = 3), &quot;)&quot;)) ) %&gt;% kableExtra::kable() x hangover not_hangover mother 62 (0.019) 4065 (4126.981) not mother 18420 (18481.981) 4091448417 (4091448355.019) Of course we want to compute the collostruction strength not only for this one lexeme, but for all that occur in our dataset. We could write a function that automatizes what we’ve done so far; but luckily, Susanne Flach has already done that for us, and developed an entire R package dedicated to collostructional analysis. It is not (yet) available on CRAN but you can download it on her website. After installing it following the instructions given there, you should be able to load it: library(collostructions) The input that the package expects is simple - just check the documentation of the individual functions, for simple collexeme analysis ?collex, where it says that the first argument to be provided to the function should be “[a] data frame with types in a construction in column 1 (WORD), construction frequencies in column 2 (CXN.FREQ) and corpus frequencies in column 3 (CORP.FREQ).” Thus, we have to create a joint dataframe first - currently, the corpus frequencies for the first two cells are in the dataframe named moa, while the construction frequencies for the two remaining cells are in the dataframe encow. We first use the table function to get a dataframe containing the frequency of each lexeme in the [mother of all N] construction… # frequency within &quot;other of all&quot; tbl1 &lt;- moa %&gt;% select(lemma) %&gt;% # select the &quot;lemma&quot; column table %&gt;% # tabulate as_tibble() %&gt;% # convert to dataframe setNames(c(&quot;Lemma&quot;, &quot;Freq_mother&quot;)) # set the column names … and then join this table with the list of corpus frequencies, encow: tbl1 &lt;- left_join(tbl1, encow) ## Joining, by = &quot;Lemma&quot; Note that in this case, we don’t have to explicitly specify by which columns we want to join the data because there’s a column called “Lemma” in both dataframes, so the left_join function automatically uses the “Lemma” column as the variable to join by. If the columns had different names, say “Lemma” and “lemma”, we would have had to specify the columns explicitly, e.g. left_join(tbl1, encow, by = c(\"lemma\" = \"Lemma\")). Note that tbl1 conforms to the input required by the collex function, as cited above: The word in the first column, the construction frequency of each lexical item in the second, and its corpus frequency in the third. With the numbers in the second and third column of our new dataframe tbl1, we can fill the first two cells of our contingency table. Now you may wonder what happened to the third and fourth cells. Well, both can be easily computed as soon as we have the total number of words belonging to a certain category (in our case, nouns) in the corpus. We have seen above that ENCOW16A contains 4091470964 nouns. We can provide this value to collex as the corpsize argument. Thus, we have everything we need to do the simple collexeme analysis. However, the following line of code throws an error: collex(tbl1, corpsize = 4091470964) ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Lemma Freq_mother Freq ## 900 megamover 2 NA ## 96 beamprop 1 NA ## 164 branding 1 NA ## 228 categorical 1 NA ## 274 clear-cutter 1 NA ## 312 complementary 1 NA ## 386 cryptic 1 NA ## 502 dying 1 NA ## 504 earach 1 NA ## 646 gap-and-crap 1 NA ## 816 kicking 1 NA ## 873 mal 1 NA ## 994 nugshot 1 NA ## 1009 omnishamble 1 NA ## 1042 parliament-squarer 1 NA ## 1166 rapid 1 NA ## 1170 rattling 1 NA ## 1247 rollicking 1 NA ## 1310 shellacking 1 NA ## 1477 tamtram 1 NA ## 1506 thighburner 1 NA ## 1574 unclogging 1 NA ## 1575 uncontainable 1 NA ## 1578 unpleaseable 1 NA ## 1632 whooping 1 NA ## 1646 wobbly 1 NA ## Error in collex(tbl1, corpsize = 4091470964): ## Your input contains the above lines with incomplete cases. Why? The answer is simple, but maybe not obvious. In the moa dataframe, we are dealing with manually annotated data. Tobias and I have corrected the lemmatization of the individual lexemes because we wanted to have a reliable picture of their frequency in the construction. This is not possible for the entire corpus of course. This is why there are some lexemes that are attested in our [mother of all N] dataset, but not in the corpus (because there, they are lemmatized as “” or with a wrong lemma). As the error message above shows, they have a frequency value of NA. We can replace NA by 0 in such cases using the helpful replace_na function: tbl1 &lt;- tbl1 %&gt;% replace_na(list(Freq = 0)) Will this help? collex(tbl1, corpsize = 4091470964) ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Lemma Freq_mother Freq ## 900 megamover 2 0 ## 96 beamprop 1 0 ## 164 branding 1 0 ## 228 categorical 1 0 ## 274 clear-cutter 1 0 ## 312 complementary 1 0 ## 386 cryptic 1 0 ## 502 dying 1 0 ## 504 earach 1 0 ## 646 gap-and-crap 1 0 ## 816 kicking 1 0 ## 873 mal 1 0 ## 994 nugshot 1 0 ## 1009 omnishamble 1 0 ## 1042 parliament-squarer 1 0 ## 1166 rapid 1 0 ## 1170 rattling 1 0 ## 1247 rollicking 1 0 ## 1310 shellacking 1 0 ## 1477 tamtram 1 0 ## 1506 thighburner 1 0 ## 1574 unclogging 1 0 ## 1575 uncontainable 1 0 ## 1578 unpleaseable 1 0 ## 1632 whooping 1 0 ## 1646 wobbly 1 0 ## Error in collex(tbl1, corpsize = 4091470964): ## For the above item(s), the frequency in the construction is larger than the frequency in the corpus. Not really, as the function complains about the same rows again - rightly so, because it doesn’t make much sense to look at items that are, at least according to the lemmatization of the corpus, not attested at all in our database. It would be like counting the number of apples drawn from a bag of pears. There are several ways to solve this problem. If we wanted to have very precise results, we could manually query the corpus for the tokens that are only attested in the [mother of all N] database, ideally taking spelling variants into account, thus obtaining the corpus frequency for each of the affected lexical items individually. Given the low frequency of the affected lemmas in the construction, however, this seems rather unnecessary. Personally, I tend to exclude those items (and make this transparent when I’m reporting the results in a paper). tbl1 &lt;- tbl1[which(tbl1$Freq_mother &lt;= tbl1$Freq),] Now we have a table that should work as input for collex: tbl1 %&gt;% collex() %&gt;% DT::datatable() ## Warning in if (class(x) == &quot;list&quot;) {: the condition has length &gt; 1 and ## only the first element will be used Et voilà, we have a collexeme table we can work with - for example, by interpreting the results: Among other things, it is interesting to see that the items most strongly attracted to the construction tend to have a negative semantic prosody, with many of them being rather colloquiual, which might contribute to the “extravagant” nature of the construction. "],["references.html", "5 References", " 5 References Bergs, Alexander. 2019. What, if anything, is linguistic creativity? Gestalt Theory 41(2). 173–183. Flach, Susanne. 2017. collostructions: An R Implementation for the Family of Collostructional Methods. Gries, Stefan Th. 2015. More (old and new) misunderstandings of collostructional analysis: On Schmid and Küchenhoff (2013). Cognitive Linguistics 26(3). 505–536. Gries, Stefan Th. 2019. 15 years of collostructions: Some long overdue additions/corrections (to/of actually all sorts of corpus-linguistics measures). International Journal of Corpus Linguistics 24(3). 385–412. Gries, Stefan Th. &amp; Anatol Stefanowitsch. 2004. Extending Collostructional Analysis: A Corpus-Based Perspective on “Alternations.” International Journal of Corpus Linguistics 9(1). 97–129. Hilpert, Martin. 2010. The force dynamics of English complement clauses: A collostructional analysis. In Dylan Glynn &amp; Kerstin Fischer (eds.), Quantitative Methods in Cognitive Semantics: Corpus-Driven Approaches, 155–178. Berlin, New York: De Gruyter. Hilpert, Martin. 2012. Diachronic Collostructional Analysis Meets the Noun Phrase: Studying MANY A NOUN in COHA. In Terttu Nevalainen &amp; Elizabeth Closs Traugott (eds.), The Oxford Handbook of the History of English, 233–244. Oxford: Oxford University Press. Küchenhoff, Helmut &amp; Hans-Jörg Schmid. 2015. Reply to “More (old and new) misunderstandings of collostructional analysis: On Schmid &amp; Küchenhoff” by Stefan Th. Gries. Cognitive Linguistics 26(3). 537–547. Schäfer, Roland &amp; Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool Chain. In Nicoletta Calzolari, Khalid Choukri, Terry Declerck, Mehmet Uğur Doğan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk &amp; Stelios Piperidis (eds.), Proceedings of LREC 2012, 486–493. Schmid, Hans-Jörg &amp; Helmut Küchenhoff. 2013. Collostructional Analysis and other Ways of Measuring Lexicogrammatical Attraction: Theoretical Premises, Practical Problems and Cognitive Underpinnings. Cognitive Linguistics 24(3). 531–577. Stefanowitsch, Anatol. 2013. Collostructional Analysis. In Thomas Hoffmann &amp; Graeme Trousdale (eds.), The Oxford Handbook of Construction Grammar, 290–306. Oxford: Oxford University Press. Stefanowitsch, Anatol &amp; Stefan Th. Gries. 2003. Collostructions: Investigating the Interaction of Words and Constructions. International Journal of Corpus Linguistics 8(2). 209–243. Ungerer, Tobias &amp; Stefan Hartmann. 2020. Delineating extravagance: Assessing speakers’ perceptions of imaginative constructional patterns. Belgian Journal of Linguistics 34. 345–356. Wiechmann, Daniel. 2008. On the computation of collostruction strength: Testing measures of association as expressions of lexical bias. Corpus Linguistics and Linguistic Theory 4(2). 253–290. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
